{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "recurrent_recommender.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! git clone git@github.com:yupopov/fantlab-recommender-system.git\n",
        "import os\n",
        "os.chdir('fantlab-recommender-system')"
      ],
      "metadata": {
        "id": "Yb_zdSM00gs-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "047593a6-5c1a-4e7a-962a-0b8de3cb371a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'fantlab-recommender-system'...\n",
            "Warning: Permanently added the ED25519 host key for IP address '140.82.113.3' to the list of known hosts.\n",
            "remote: Enumerating objects: 578, done.\u001b[K\n",
            "remote: Counting objects: 100% (175/175), done.\u001b[K\n",
            "remote: Compressing objects: 100% (119/119), done.\u001b[K\n",
            "remote: Total 578 (delta 112), reused 112 (delta 53), pack-reused 403\u001b[K\n",
            "Receiving objects: 100% (578/578), 545.34 MiB | 27.61 MiB/s, done.\n",
            "Resolving deltas: 100% (307/307), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rw0QV-MW1Wlw"
      },
      "outputs": [],
      "source": [
        "! git config --global user.email \"yuri.ppv@gmail.com\"\n",
        "! git config --global user.name \"Yury Popov\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import gzip\n",
        "import json\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import Module, Embedding, LSTM, RNN, GRU, Linear, Sequential, Dropout, \\\n",
        "    CrossEntropyLoss\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from scipy.sparse import coo_matrix, csr_matrix, csc_matrix, load_npz\n",
        "! pip install lightfm\n",
        "\n",
        "from src.preprocessing.datasets import RNNDatasetMaker\n",
        "from src.models.get_top_k_predictions_with_label import get_top_k_predictions_with_labels\n",
        "from src.models.rnn_recommender import RecurrentLanguageModel, RecurrentRecommender\n",
        "from src.models.trainer import Trainer\n",
        "from src.experiments.run_n_experiments import plot_experiments, run_experiment, run_n_experiments\n"
      ],
      "metadata": {
        "id": "45qTRQ3J0sK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_dataset_maker = RNNDatasetMaker()\n",
        "rnn_dataset = rnn_dataset_maker.get_rnn_dataset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5D5ZucYk2kap",
        "outputId": "d3849f13-e2ad-4bda-dfa2-76c8eee0eeda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading marks...\n",
            "Loading embeddings...\n",
            "Done.\n",
            "Stats before filtering by date:\n",
            "Marks: 7368410\tUnique titles: 23867\tUnique users: 61739\n",
            "\n",
            "Deleting marks dated before 2012...\n",
            "Stats after filtering by date:\n",
            "Marks: 5056872\tUnique titles: 23867\tUnique users: 47578\n",
            "\n",
            "Splitting the marks by 2020-12-07 00:00:00...\n",
            "Train set stats:\n",
            "Marks: 4551770\tUnique titles: 23768\tUnique users: 43998\n",
            "\n",
            "Test set stats:\n",
            "Marks: 505102\tUnique titles: 23715\tUnique users: 13440\n",
            "\n",
            "Dropping works with less than 50 marks in the train set...\n",
            "Train set stats:\n",
            "Marks: 4342747\tUnique titles: 18130\tUnique users: 43774\n",
            "\n",
            "Dropping works with less than 10 marks in the test set...\n",
            "Test set stats:\n",
            "Marks: 446391\tUnique titles: 12199\tUnique users: 13264\n",
            "\n",
            "Dropping works with interactions only in the train or the test set...\n",
            "Train set stats:\n",
            "Marks: 3778163\tUnique titles: 11230\tUnique users: 42840\n",
            "\n",
            "Test set stats:\n",
            "Marks: 407321\tUnique titles: 11230\tUnique users: 12939\n",
            "\n",
            "Dropping users with less than 10 marks in the train set...\n",
            "Stats after filtering:\n",
            "Marks: 3730468\tUnique titles: 11230\tUnique users: 28211\n",
            "\n",
            "Dropping users from the test set with no marks in the train set...\n",
            "Stats after filtering:\n",
            "Marks: 262382\tUnique titles: 11230\tUnique users: 8748\n",
            "\n",
            "Constructing test dataset...\n",
            "Constructing train and val RNN datasets...\n",
            "Total 25389 users, 179271 sequences of length <= 20 in train set.\n",
            "Total 2822 users, 20106 sequences of length <= 20 in valid set.\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model training"
      ],
      "metadata": {
        "id": "8b0MjUroXn6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_preds(model, dataset, k=20):\n",
        "    recommender = RecurrentRecommender(model, dataset.pred_dataset)\n",
        "    top_k_preds, labels = get_top_k_predictions_with_labels(recommender,\n",
        "                                  dataset.test_interactions,\n",
        "                                  dataset.train_interactions, k=k\n",
        "                                  )\n",
        "    precision = labels.sum(axis=1).mean()/k\n",
        "    print(f'Prediction precision: {precision:.4f}')\n",
        "\n",
        "    return top_k_preds"
      ],
      "metadata": {
        "id": "kgoL5nC5RBZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net_config = {\n",
        "    \"freeze_embs\": True,\n",
        "    \"cell_type\": \"GRU\",\n",
        "    \"cell_dropout\": 0.5,\n",
        "    \"num_layers\": 2,\n",
        "    \"hidden_size\": 256,\n",
        "    \"out_activation\": \"relu\",\n",
        "    \"out_dropout\": 0.5,\n",
        "    \"out_sizes\": [500, 500]}\n",
        "    \n",
        "trainer_config = {\n",
        "    \"n_epochs\": 30,\n",
        "    \"batch_size\": 256,\n",
        "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    'optimizer_cls': torch.optim.Adam,\n",
        "    'optimizer_params': {\n",
        "        'lr': 1e-2,\n",
        "        'weight_decay': 1e-6,\n",
        "    },\n",
        "    'scheduler_cls': torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
        "    'scheduler_params': {\n",
        "        'patience': 0,\n",
        "        'verbose': True\n",
        "    }, \n",
        "}\n",
        "\n",
        "train_dataset = rnn_dataset.train_dataset\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=trainer_config['batch_size'], collate_fn=train_dataset.collate_fn, shuffle=True)\n",
        "\n",
        "\n",
        "val_dataset = rnn_dataset.val_dataset\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=trainer_config['batch_size'], collate_fn=train_dataset.collate_fn, shuffle=True)\n",
        "\n",
        "model = RecurrentLanguageModel(net_config, rnn_dataset.item_vocab, rnn_dataset.embs)\n",
        "\n",
        "trainer = Trainer(trainer_config)\n",
        "\n",
        "trainer.fit(model, train_dataloader, val_dataloader)\n",
        "\n",
        "preds = make_preds(model, rnn_dataset)\n",
        "\n"
      ],
      "metadata": {
        "id": "mqE3p_hZrHOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exp_res = run_n_experiments(rnn_dataset, net_config, trainer_config, bottom=5, top=8, n_experiments=1)"
      ],
      "metadata": {
        "id": "zUDj5fJ6ILEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_experiments(exp_res.keys(), exp_res, bottom=7.6, n_experiments=1)"
      ],
      "metadata": {
        "id": "pCTJ5VhYZ_dq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}